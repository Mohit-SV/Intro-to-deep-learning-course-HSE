{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "RMSProp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXp613VXDo0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "6a1ab9f1-473e-44cb-f264-adea48df4cb2"
      },
      "source": [
        "! shred -u setup_google_colab.py\n",
        "! wget https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "setup_google_colab.setup_week2_honor()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-23 15:53:53--  https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3636 (3.6K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "\rsetup_google_colab.   0%[                    ]       0  --.-KB/s               \rsetup_google_colab. 100%[===================>]   3.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-23 15:53:53 (56.5 MB/s) - ‘setup_google_colab.py’ saved [3636/3636]\n",
            "\n",
            "**************************************************\n",
            "inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "**************************************************\n",
            "cifar-10-batches-py.tar.gz\n",
            "**************************************************\n",
            "mnist.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doGYCF6sFS9A",
        "colab_type": "text"
      },
      "source": [
        "### Your very own neural network\n",
        "\n",
        "In this notebook we're going to build a neural network using naught but pure numpy and steel nerves. It's going to be fun, I promise!\n",
        "\n",
        "<img src=\"frankenstein.png\" style=\"width:20%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibj6DEVi58en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import tqdm_utils\n",
        "import download_utils\n",
        "\n",
        "# use the preloaded keras datasets and models\n",
        "download_utils.link_all_keras_resources()\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s-y7LAGD-_x",
        "colab_type": "text"
      },
      "source": [
        "Here goes our main class: a layer that can do .forward() and .backward() passes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r1JbuneD_Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "    \n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "    \n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "    \n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
        "        # A dummy layer does nothing\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
        "        \"\"\"\n",
        "        # A dummy layer just returns whatever it gets as input.\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "        \n",
        "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
        "        \n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "        \n",
        "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "        \n",
        "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
        "        num_units = input.shape[1]\n",
        "        \n",
        "        d_layer_d_input = np.eye(num_units)\n",
        "        \n",
        "        return np.dot(grad_output, d_layer_d_input) # chain rule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HG2WAJWD_Uf",
        "colab_type": "text"
      },
      "source": [
        "### The road ahead\n",
        "\n",
        "We're going to build a neural network that classifies MNIST digits. To do so, we'll need a few building blocks:\n",
        "- Dense layer - a fully-connected layer, $f(X)=W \\cdot X + \\vec{b}$\n",
        "- ReLU layer (or any other nonlinearity you want)\n",
        "- Loss function - crossentropy\n",
        "- Backprop algorithm - a stochastic gradient descent with backpropageted gradients\n",
        "\n",
        "Let's approach them one at a time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VWuzuiED_fC",
        "colab_type": "text"
      },
      "source": [
        "### Nonlinearity layer\n",
        "\n",
        "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhZ_TP7QD_qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
        "        return np.maximum(input, 0)\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
        "        relu_grad = input > 0\n",
        "        return grad_output*relu_grad        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxbdN5BTD_13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some tests\n",
        "from util import eval_numerical_gradient\n",
        "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "l = ReLU()\n",
        "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
        "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
        "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
        "    \"gradient returned by your layer does not match the numerically computed gradient\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JkeBDLSEAAO",
        "colab_type": "text"
      },
      "source": [
        "#### Instant primer: lambda functions\n",
        "\n",
        "In python, you can define functions in one line using the `lambda` syntax: `lambda param1, param2: expression`\n",
        "\n",
        "For example: `f = lambda x, y: x+y` is equivalent to a normal function:\n",
        "\n",
        "```\n",
        "def f(x,y):\n",
        "    return x+y\n",
        "```\n",
        "For more information, click [here](http://www.secnetix.de/olli/Python/lambda_functions.hawk).    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwbI8Ah9EAl3",
        "colab_type": "text"
      },
      "source": [
        "### Dense layer\n",
        "\n",
        "Now let's build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\n",
        "\n",
        "A dense layer applies affine transformation. In a vectorized form, it can be described as:\n",
        "$$f(X)= W \\cdot X + \\vec b $$\n",
        "\n",
        "Where \n",
        "* X is an object-feature matrix of shape [batch_size, num_features],\n",
        "* W is a weight matrix [num_features, num_outputs] \n",
        "* and b is a vector of num_outputs biases.\n",
        "\n",
        "Both W and b are initialized during layer creation and updated each time backward is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjteTIVlEAzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # initialize weights with small random numbers. We use normal initialization, \n",
        "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "        \n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output units]\n",
        "        \"\"\"\n",
        "        return input @ self.weights + self.biases\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        \n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        grad_input = grad_output @ self.weights.T\n",
        "        \n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        grad_weights = input.T @ grad_output\n",
        "        grad_biases = grad_output.sum(axis=0) \n",
        "        \n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        # Here we perform a stochastic gradient descent step. \n",
        "        # Later on, you can try replacing that with something better.\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "        \n",
        "        return grad_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_aRFXHAEA_w",
        "colab_type": "text"
      },
      "source": [
        "### Testing the dense layer\n",
        "\n",
        "Here we have a few tests to make sure your dense layer works properly. You can just run them, get 3 \"well done\"s and forget they ever existed.\n",
        "\n",
        "... or not get 3 \"well done\"s and go fix stuff. If that is the case, here are some tips for you:\n",
        "* Make sure you compute gradients for W and b as __sum of gradients over batch__, not mean over gradients. Grad_output is already divided by batch size.\n",
        "* If you're debugging, try saving gradients in class fields, like \"self.grad_w = grad_w\" or print first 3-5 weights. This helps debugging.\n",
        "* If nothing else helps, try ignoring tests and proceed to network training. If it trains alright, you may be off by something that does not affect network training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7no7L1TtEBPI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b522d718-c94d-48c7-d361-6822c35c50f9"
      },
      "source": [
        "l = Dense(128, 150)\n",
        "\n",
        "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
        "    \"The initial weights must have zero mean and small variance. \"\\\n",
        "    \"If you know what you're doing, remove this assertion.\"\n",
        "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
        "\n",
        "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
        "l = Dense(3,4)\n",
        "\n",
        "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
        "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
        "l.biases = np.linspace(-1,1,4)\n",
        "\n",
        "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
        "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHMQvFOFEBfR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c99b6ba-b172-4013-c640-b7d687dc790a"
      },
      "source": [
        "# To test the grads, we use gradients obtained via finite differences\n",
        "\n",
        "from util import eval_numerical_gradient\n",
        "\n",
        "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "l = Dense(32,64,learning_rate=0)\n",
        "\n",
        "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
        "grads = l.backward(x,np.ones([10,64]))\n",
        "\n",
        "assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), \"input gradient does not match numeric grad\"\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMv53deBEBsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d06c204a-a7dd-4862-d71a-4a006a3b5752"
      },
      "source": [
        "#test gradients w.r.t. params\n",
        "def compute_out_given_wb(w,b):\n",
        "    l = Dense(32,64,learning_rate=1)\n",
        "    l.weights = np.array(w)\n",
        "    l.biases = np.array(b)\n",
        "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "    return l.forward(x)\n",
        "    \n",
        "def compute_grad_by_params(w,b):\n",
        "    l = Dense(32,64,learning_rate=1)\n",
        "    l.weights = np.array(w)\n",
        "    l.biases = np.array(b)\n",
        "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "    l.backward(x,np.ones([10,64]) / 10.)\n",
        "    return w - l.weights, b - l.biases\n",
        "    \n",
        "w,b = np.random.randn(32,64), np.linspace(-1,1,64)\n",
        "\n",
        "numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w )\n",
        "numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b )\n",
        "grad_w,grad_b = compute_grad_by_params(w,b)\n",
        "\n",
        "assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
        "assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "560HBej5EB5_",
        "colab_type": "text"
      },
      "source": [
        "### The loss function\n",
        "\n",
        "Since we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\n",
        "\n",
        "If you write down the expression for crossentropy as a function of softmax logits (a), you'll see:\n",
        "\n",
        "$$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
        "\n",
        "If you take a closer look, ya'll see that it can be rewritten as:\n",
        "\n",
        "$$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $$\n",
        "\n",
        "It's called Log-softmax and it's better than naive log(softmax(a)) in all aspects:\n",
        "* Better numerical stability\n",
        "* Easier to get derivative right\n",
        "* Marginally faster to compute\n",
        "\n",
        "So why not just use log-softmax throughout our computation and never actually bother to estimate probabilities.\n",
        "\n",
        "Here you are! We've defined the both loss functions for you so that you could focus on neural network part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_w3xrakECJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "    \n",
        "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "    \n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "    \n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "    \n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0hAL57XECVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = np.linspace(-1,1,500).reshape([50,10])\n",
        "answers = np.arange(50)%10\n",
        "\n",
        "softmax_crossentropy_with_logits(logits,answers)\n",
        "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
        "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
        "\n",
        "assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0), \"The reference implementation has just failed. Someone has just changed the rules of math.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BSii0j1ECjQ",
        "colab_type": "text"
      },
      "source": [
        "### Full network\n",
        "\n",
        "Now let's combine what we've just built into a working neural network. As we announced, we're gonna use this monster to classify handwritten digits, so let's get them loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHKmHda7ECu3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "f319a49c-7c13-4583-beea-4cffade85493"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from preprocessed_mnist import load_dataset\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
        "\n",
        "plt.figure(figsize=[6,6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAF1CAYAAAAjhLvUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnP39sO6Eg_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = []\n",
        "network.append(Dense(X_train.shape[1],100))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(100,200))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(200,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHlIZS1MEhKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute activations of all network layers by applying them sequentially.\n",
        "    Return a list of activations for each layer. \n",
        "    Make sure last activation corresponds to network logits.\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    input = X\n",
        "\n",
        "    for layer in network:\n",
        "        input = layer.forward(input)\n",
        "        activations.append(input)\n",
        "        \n",
        "    assert len(activations) == len(network)\n",
        "    return activations\n",
        "\n",
        "def predict(network,X):\n",
        "    \"\"\"\n",
        "    Compute network predictions.\n",
        "    \"\"\"\n",
        "    logits = forward(network,X)[-1]\n",
        "    return logits.argmax(axis=-1)\n",
        "\n",
        "def train(network,X,y):\n",
        "    \"\"\"\n",
        "    Train your network on a given batch of X and y.\n",
        "    You first need to run forward to get all layer activations.\n",
        "    Then you can run layer.backward going from last to first layer.\n",
        "    \n",
        "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get the layer activations\n",
        "    layer_activations = forward(network,X)\n",
        "    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n",
        "    logits = layer_activations[-1]\n",
        "    \n",
        "    # Compute the loss and the initial gradient\n",
        "    loss = softmax_crossentropy_with_logits(logits,y)\n",
        "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
        "\n",
        "    for activ, layer in zip(reversed(layer_activations[:-1]), reversed(network)):\n",
        "        loss_grad = layer.backward(activ, loss_grad)\n",
        "        \n",
        "    return np.mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DveknrmrEhUh",
        "colab_type": "text"
      },
      "source": [
        "Instead of tests, we provide you with a training loop that prints training and validation accuracies on every epoch.\n",
        "\n",
        "If your implementation of forward and backward are correct, your accuracy should grow from 90~93% to >97% with the default network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsucSRWrEheF",
        "colab_type": "text"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "As usual, we split data into minibatches, feed each such minibatch into the network and update weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQgXb0nKEhrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in tqdm_utils.tqdm_notebook_failsafe(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71-23MRTEoNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "fd0e0581-290a-44d9-8d0a-cf4facdf71d9"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []\n",
        "\n",
        "for epoch in range(25):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
        "        train(network,x_batch,y_batch)\n",
        "    \n",
        "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
        "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
        "    \n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",train_log[-1])\n",
        "    print(\"Val accuracy:\",val_log[-1])\n",
        "    plt.plot(train_log,label='train accuracy')\n",
        "    plt.plot(val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 24\n",
            "Train accuracy: 0.89598\n",
            "Val accuracy: 0.9045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvmfqS1W58fJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Peer review - optimization (RMSProp)\n",
        "\n",
        "What I'll do here is implement the RMSProp optimization method. RMSProp modifies the standard gradient descent by dividing the gradient in the updating step by a exponential moving average of squared gradients. In math,\n",
        "$$\n",
        "s_t = \\beta \\cdot s_{t-1} + (1-\\beta) \\cdot dw_t^2 \n",
        "$$\n",
        "$$\n",
        "w_t = w_{t-1} - \\eta \\cdot \\frac{dw_t}{\\sqrt{s_t + \\epsilon}}\n",
        "$$\n",
        "\n",
        "Here $w_t$ is the parameter (such as the weights or biases) we want to update, $\\beta$ is the moving average parameter, $dw$ is the gradient of out parameter, and $\\epsilon$ is just some constant for numerical stability.\n",
        "\n",
        "Just to be clear, the squaring (in the upper formula) and divising (in the lower formula) is element wise (for matrices), there is no matrix multiplication involved.\n",
        "\n",
        "Intuitively, what this is trying to achieve is to prevent vanishing and exploding gradients, as the gradients are kind of normalized by their size.\n",
        "\n",
        "## Implementation\n",
        "\n",
        "I won't overcomplicate things here - I will just create a new Dense layer class which stores these parameters and does the updates, as before. I will use the standard values for the parameters, which are\n",
        "$$\n",
        "\\beta = 0.9,\\ \\epsilon = 10^{-7},\\ \\eta = 0.001\n",
        "$$\n",
        "As you can see, the learning rate is much lower than in SGD case. This is needed as otherwise we run into numeric issues (some values become too close to zero).\n",
        "\n",
        "The initial value $s_0$ will be set to 1. Most of stuff are kept from the exercise notebook, the only real change is in the `Dense` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b6al-z58fL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################\n",
        "# New Dense layer\n",
        "##########################################\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1, \n",
        "                 beta=0.9, epsilon = 1e-7, optimizer = 'SGD'):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation: f(x) = x @ w + b\n",
        "        \n",
        "        The parameter updating can be done based on SGD or RMSProp\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        input_units: number of input units\n",
        "        output_units: number of output units\n",
        "        learning_rate: learning rate. Good defaults are 0.1 for SGD, and 0.001 for RMSProp\n",
        "        beta: the moving average parameter for RMSProp\n",
        "        epsilon: the numerical stability parameter for RMSProp\n",
        "        optimizer: can be either 'SGD' or 'RMSProp'\n",
        "        \"\"\"\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        # initialize weights with small random numbers. We use normal initialization, \n",
        "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "        \n",
        "        # Initialize the exponential averages to zero\n",
        "        self.rms_weights = np.ones_like(self.weights)\n",
        "        self.rms_biases = np.ones_like(self.biases)\n",
        "        \n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output units]\n",
        "        \"\"\"\n",
        "        \n",
        "        return input @ self.weights + self.biases\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        \n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        grad_input = grad_output @ self.weights.T\n",
        "        \n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        grad_weights = input.T @ grad_output\n",
        "        grad_biases = grad_output.sum(axis=0) \n",
        "        \n",
        "        beta, lr, eps = self.beta, self.learning_rate, self.epsilon\n",
        "        rms_w, rms_b = self.rms_weights, self.rms_biases\n",
        "\n",
        "        # Here we perform the RMSProp update step. \n",
        "        if self.optimizer == 'RMSProp':\n",
        "            # First, let's update the rms values\n",
        "            self.rms_weights = beta * rms_w + (1-beta) * grad_weights**2\n",
        "            self.rms_biases = beta * rms_b + (1-beta) * grad_biases**2\n",
        "\n",
        "            # Then update parameters\n",
        "            self.weights = self.weights - lr*grad_weights/np.sqrt(rms_w + eps)\n",
        "            self.biases = self.biases - lr*grad_biases/np.sqrt(rms_b + eps)\n",
        "        \n",
        "        # SGD update step\n",
        "        elif self.optimizer == 'SGD':\n",
        "            self.weights = self.weights - lr * grad_weights\n",
        "            self.biases = self.biases - lr * grad_biases\n",
        "        \n",
        "        return grad_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvmLON1V58fd",
        "colab_type": "text"
      },
      "source": [
        "## Comparison with SGD\n",
        "\n",
        "I'll show that RMSProp enables the model to learn faster than with SGD. To do that, I'll train one the same data as before, and compare the validation accuracies in each epoch. SGD will keep the learning rate of 0.1 (if we also gave it 0.001 learning rate if would basically not learn anything). You'll see that RMSProp model reaches higher accuracies much faster.\n",
        "\n",
        "The results should be taken with a grain of salt - after all, the learning rates are different for both, perhaps by tuning the learning rate of SGD we could have also achieved a better outcome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckgIe14t58fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in tqdm_utils.tqdm_notebook_failsafe(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]\n",
        "        \n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9gERpWp58fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################\n",
        "# RMSProp network\n",
        "##################\n",
        "\n",
        "LR_MSPROP = 0.001\n",
        "network_rmsprop = []\n",
        "network_rmsprop.append(Dense(X_train.shape[1],100, optimizer='RMSProp', learning_rate=LR_MSPROP))\n",
        "network_rmsprop.append(ReLU())\n",
        "network_rmsprop.append(Dense(100,200, optimizer='RMSProp', learning_rate=LR_MSPROP))\n",
        "network_rmsprop.append(ReLU())\n",
        "network_rmsprop.append(Dense(200,10, optimizer='RMSProp', learning_rate=LR_MSPROP))\n",
        "\n",
        "train_log_rmsprop = []\n",
        "val_log_rmsprop = []\n",
        "\n",
        "##################\n",
        "# SGD network\n",
        "##################\n",
        "\n",
        "network_sgd = []\n",
        "network_sgd.append(Dense(X_train.shape[1],100))\n",
        "network_sgd.append(ReLU())\n",
        "network_sgd.append(Dense(100,200))\n",
        "network_sgd.append(ReLU())\n",
        "network_sgd.append(Dense(200,10))\n",
        "\n",
        "train_log_sgd = []\n",
        "val_log_sgd = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNcVDd4N58fn",
        "colab_type": "code",
        "outputId": "88e92f55-f717-4d13-f12a-14fcc83dda68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "################\n",
        "# Train both\n",
        "################\n",
        "NUM_EPOCHS = 25\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
        "        train(network_rmsprop,x_batch,y_batch)\n",
        "        train(network_sgd,x_batch,y_batch)\n",
        "    \n",
        "    train_log_rmsprop.append(np.mean(predict(network_rmsprop,X_train)==y_train))\n",
        "    val_log_rmsprop.append(np.mean(predict(network_rmsprop,X_val)==y_val))\n",
        "\n",
        "    train_log_sgd.append(np.mean(predict(network_sgd,X_train)==y_train))\n",
        "    val_log_sgd.append(np.mean(predict(network_sgd,X_val)==y_val))\n",
        "\n",
        "    clear_output()\n",
        "    plt.plot(val_log_rmsprop,label='RMSProp')\n",
        "    plt.plot(val_log_sgd,label='SGD')\n",
        "    plt.title('Validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}