{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment with L2 Regularization and Xavier Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tqdm_utils\n",
    "import download_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the preloaded keras datasets and models\n",
    "download_utils.link_all_keras_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.01, l2_regularization_alpha = None):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weights with xavier init\n",
    "        self.weights = np.random.randn(input_units, output_units)*(2/(input_units+output_units))\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.l2_regularization_alpha = l2_regularization_alpha\n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        \n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = np.dot(input.T, grad_output) \n",
    "        grad_biases = np.sum(grad_output, axis = 0) \n",
    "        \n",
    "        if self.l2_regularization_alpha:\n",
    "            grad_weights += 2 * self.l2_regularization_alpha * self.weights\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
    "        # <your code. Try np.maximum>\n",
    "        return np.maximum(input, np.zeros(input.shape))\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    \"\"\"\n",
    "    Compute activations of all network layers by applying them sequentially.\n",
    "    Return a list of activations for each layer. \n",
    "    Make sure last activation corresponds to network logits.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X\n",
    "\n",
    "    for layer in network:\n",
    "        output = layer.forward(input)\n",
    "        activations.append(output)\n",
    "        input = output\n",
    "        \n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network,X):\n",
    "    \"\"\"\n",
    "    Compute network predictions.\n",
    "    \"\"\"\n",
    "    logits = forward(network,X)[-1]\n",
    "    return logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in tqdm_utils.tqdm_notebook_failsafe(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(network,X,y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    Then you can run layer.backward going from last to first layer.\n",
    "    \n",
    "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    dense_layers = [dense for dense in network if isinstance(dense,Dense)]\n",
    "    for dl in dense_layers:\n",
    "        if dl.l2_regularization_alpha:\n",
    "            loss += dl.l2_regularization_alpha * np.sum(np.square(dl.weights))\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    # <your code: propagate gradients through the network>\n",
    "    grad_output = loss_grad\n",
    "    layer_inputs = layer_inputs[:-1]\n",
    "    for input,layer in zip(layer_inputs[::-1],network[::-1]):\n",
    "        grad_output = layer.backward(input,grad_output)\n",
    "        \n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAF1CAYAAADx1LGMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e33bcb828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_regularization_alpha = 0.001\n",
    "\n",
    "network = []\n",
    "network.append(Dense(X_train.shape[1],100, l2_regularization_alpha = l2_regularization_alpha))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(100,200, l2_regularization_alpha = l2_regularization_alpha))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(200,10, l2_regularization_alpha = l2_regularization_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log_reg = []\n",
    "val_log_reg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log_reg.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log_reg.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log_reg[-1])\n",
    "    print(\"Val accuracy:\",val_log_reg[-1])\n",
    "    plt.plot(train_log_reg,label='train accuracy')\n",
    "    plt.plot(val_log_reg,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
